{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c004938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "from bayes_opt import BayesianOptimization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score,mean_absolute_error,explained_variance_score,mean_squared_error,median_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "import openpyxl\n",
    "from keras.layers import Input, Dense, Conv1D, Lambda\n",
    "from keras.models import Model, Sequential\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e5b50f",
   "metadata": {},
   "source": [
    "## Pre processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15e59385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepTrainSet(filename):\n",
    "    all_Data = pd.read_excel(filename, engine='openpyxl', index_col=0)\n",
    "    #print(all_Data.head(2))\n",
    "    seq_data = []\n",
    "    str_data=[]\n",
    "    exp_out = []\n",
    "    for index,item in all_Data.iterrows():\n",
    "        seq_values = []\n",
    "        str_values = []\n",
    "        for key in['perjxW', 'sum_hydW','sumxxcW', 'walx','wbzd','H','numB','numA']:\n",
    "            str_values.append(item[key])\n",
    "\n",
    "        for key in['GA','GC','AA', 'CA','X','perX']:\n",
    "            seq_values.append(item[key])\n",
    "        exp_out.append(item['Experimental'])\n",
    "\n",
    "        seq_data.append(seq_values)\n",
    "        str_data.append(str_values)\n",
    "    \n",
    "    return str_data,seq_data,exp_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b43d5f4",
   "metadata": {},
   "source": [
    "## Hyper parameter tuning Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcfb7100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameter tuning attempt\n",
    "def tuning(filename):\n",
    "    \n",
    "    def created_model():\n",
    "        model = tf.keras.Sequential(layers=[\n",
    "        tf.keras.layers.Dense(12, input_dim=6, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')])\n",
    "            \n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        \n",
    "        #history=model.fit(X, np.array(exp_out), verbose=1)\n",
    "        \n",
    "        #pred_fin = model.predict(X_train)\n",
    "        \n",
    "        #return pred_fin, exp_out\n",
    "        return model\n",
    "    \n",
    "    seed = 7\n",
    "    np.random.seed(7)\n",
    "    \n",
    "    str_train,seq_train,exp_out = prepTrainSet(filename)\n",
    "\n",
    "    for X_train in [str_train,seq_train]:\n",
    "        X = np.array(X_train)\n",
    "    \n",
    "    model = KerasRegressor(build_fn=created_model, verbose=0)\n",
    "    \n",
    "    \n",
    "   # define the grid search parameters\n",
    "    batch_size = [10, 20, 40, 60, 80, 100]\n",
    "    epochs = [10, 50, 100, 200]\n",
    "    param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "    grid_result = grid.fit(X, np.array(exp_out))\n",
    "    \n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b7c50a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qh/yc4gx7bn0f9_3gf83t2y8yc80000gn/T/ipykernel_86096/3119460660.py:26: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
      "  model = KerasRegressor(build_fn=created_model, verbose=0)\n",
      "2022-04-23 14:08:09.264063: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-23 14:08:09.264063: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-23 14:08:09.264064: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-23 14:08:09.264064: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_test_function.<locals>.test_function at 0x7fb71b142af0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_test_function.<locals>.test_function at 0x7fb5b4585ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_test_function.<locals>.test_function at 0x7fc95b785c10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_test_function.<locals>.test_function at 0x7ffd55682ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_test_function.<locals>.test_function at 0x7fb71b5c2af0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_test_function.<locals>.test_function at 0x7fb5b4c9e790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_test_function.<locals>.test_function at 0x7fc95c463430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_test_function.<locals>.test_function at 0x7ffd55c653a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 34.958778 using {'batch_size': 40, 'epochs': 200}\n",
      "-1.397353 (3.218117) with: {'batch_size': 10, 'epochs': 10}\n",
      "6.088290 (6.931802) with: {'batch_size': 10, 'epochs': 50}\n",
      "14.875121 (21.124999) with: {'batch_size': 10, 'epochs': 100}\n",
      "18.059123 (9.915744) with: {'batch_size': 10, 'epochs': 200}\n",
      "-5.746497 (5.051279) with: {'batch_size': 20, 'epochs': 10}\n",
      "10.203030 (5.030236) with: {'batch_size': 20, 'epochs': 50}\n",
      "12.329368 (10.377695) with: {'batch_size': 20, 'epochs': 100}\n",
      "28.001565 (16.157373) with: {'batch_size': 20, 'epochs': 200}\n",
      "-7.084646 (18.813093) with: {'batch_size': 40, 'epochs': 10}\n",
      "9.145712 (6.702134) with: {'batch_size': 40, 'epochs': 50}\n",
      "2.633755 (4.153235) with: {'batch_size': 40, 'epochs': 100}\n",
      "34.958778 (37.650106) with: {'batch_size': 40, 'epochs': 200}\n",
      "8.022015 (5.142830) with: {'batch_size': 60, 'epochs': 10}\n",
      "5.318695 (2.573435) with: {'batch_size': 60, 'epochs': 50}\n",
      "1.790025 (4.635187) with: {'batch_size': 60, 'epochs': 100}\n",
      "23.235147 (5.897667) with: {'batch_size': 60, 'epochs': 200}\n",
      "1.797403 (10.406166) with: {'batch_size': 80, 'epochs': 10}\n",
      "11.082197 (8.067262) with: {'batch_size': 80, 'epochs': 50}\n",
      "14.327227 (7.431553) with: {'batch_size': 80, 'epochs': 100}\n",
      "25.917207 (12.208413) with: {'batch_size': 80, 'epochs': 200}\n",
      "-12.129612 (12.583051) with: {'batch_size': 100, 'epochs': 10}\n",
      "2.426918 (4.684273) with: {'batch_size': 100, 'epochs': 50}\n",
      "7.950679 (4.365687) with: {'batch_size': 100, 'epochs': 100}\n",
      "17.722201 (6.612762) with: {'batch_size': 100, 'epochs': 200}\n"
     ]
    }
   ],
   "source": [
    "tuning('../data2/singleDNA.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfc926e",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2529fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuralN(filename):\n",
    "\n",
    "    str_train,seq_train,exp_out = prepTrainSet(filename)\n",
    "    \n",
    "    for X_train in [str_train,seq_train]:\n",
    "        X = np.array(X_train)\n",
    "        print(X.shape)\n",
    "        print(X)\n",
    "        \n",
    "        \n",
    "        model = tf.keras.Sequential(layers=[\n",
    "            tf.keras.layers.Dense(8, input_dim=8, kernel_initializer='normal', activation='relu'),\n",
    "            tf.keras.layers.Dense(10, kernel_initializer='normal', activation='relu'),\n",
    "            tf.keras.layers.Dense(1, kernel_initializer='normal')])\n",
    "        \n",
    "        model.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])\n",
    "        \n",
    "        history=model.fit(X, np.array(exp_out), batch_size=5, epochs=20, verbose=1, validation_split=0.2)\n",
    "        \n",
    "        pred_fin = model.predict(np.array(X_train))\n",
    "        \n",
    "        model.summary()\n",
    "        \n",
    "        #print(pred_fin)\n",
    "        \n",
    "        #printAllAccMetrics(pred_fin,exp_out)\n",
    "        \n",
    "        return pred_fin, exp_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "503a1f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printAllAccMetrics(pred_fin,exp_out):\n",
    "    exp_out = np.array(exp_out)\n",
    "    exp_out = exp_out.reshape(24,1)\n",
    "    \n",
    "    pred_fin = np.squeeze(pred_fin)\n",
    "    exp_out = np.squeeze(exp_out)\n",
    "    \n",
    "    #print(pred_fin.shape)\n",
    "    #print(exp_out.shape)\n",
    "    \n",
    "    print('Mean Absolute Error(MAE):',mean_absolute_error(pred_fin,exp_out))\n",
    "    corr, corr2 = pearsonr(y_exp,exp_out)\n",
    "    print(\"Pearson's correlation coefficient: \",corr, corr2)\n",
    "    print('R2 value:', r2_score(pred_fin,exp_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db00113e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writePreds(complex_type,pred_out,exp_out):\n",
    "    outlist = []\n",
    "    filename = ''\n",
    "    for pred,exp in zip(pred_out,exp_out):\n",
    "        outlist.append([pred,exp])\n",
    "    if complex_type == 'SS':\n",
    "        filename = '../data3/NNSS.xlsx'\n",
    "    elif complex_type == 'MISC':\n",
    "        filename = '../data3/NNMisc.xlsx'\n",
    "    elif complex_type == 'DDI':\n",
    "        filename = '../data3/NNDDI.xlsx'\n",
    "    elif complex_type == 'DDII':\n",
    "        filename = '../data3/NNDDII.xlsx'\n",
    "    else:\n",
    "        filename = '../data3/NNDDIII.xlsx'\n",
    "    \n",
    "    df = pd.DataFrame(outlist,columns=['NN','Expected'])\n",
    "    df.to_excel(filename,engine='openpyxl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51a0ff08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 8)\n",
      "[[2.17970050e+01 1.57000000e+02 1.40000000e+02 3.93257000e+04\n",
      "  9.20510000e+03 1.71800000e+03 1.40000000e+01 2.60000000e+01]\n",
      " [2.42320819e+01 9.30000000e+01 7.50000000e+01 1.62147900e+04\n",
      "  5.81933000e+03 8.58000000e+02 1.10000000e+01 1.30000000e+01]\n",
      " [2.60000000e+01 6.60000000e+01 4.90000000e+01 3.54277000e+03\n",
      "  7.94007000e+03 5.56000000e+02 1.20000000e+01 4.00000000e+00]\n",
      " [2.48704663e+01 6.20000000e+01 4.00000000e+01 7.55601000e+03\n",
      "  6.11193000e+03 5.41000000e+02 8.00000000e+00 6.00000000e+00]\n",
      " [2.51764706e+01 1.28000000e+02 8.50000000e+01 3.02051100e+04\n",
      "  6.43357000e+03 1.24100000e+03 9.00000000e+00 2.00000000e+01]\n",
      " [2.17391304e+01 5.70000000e+01 5.50000000e+01 1.04765800e+04\n",
      "  6.29316000e+03 5.74000000e+02 6.00000000e+00 7.00000000e+00]\n",
      " [2.59259259e+01 1.33000000e+02 1.29000000e+02 1.70962000e+04\n",
      "  1.59119900e+04 1.21700000e+03 2.10000000e+01 1.10000000e+01]\n",
      " [2.46644295e+01 1.76000000e+02 1.38000000e+02 4.13232500e+04\n",
      "  1.15666300e+04 1.71100000e+03 2.00000000e+01 2.80000000e+01]]\n",
      "Epoch 1/20\n",
      "2/2 [==============================] - 1s 201ms/step - loss: 43.7920 - mse: 43.7920 - mae: 6.2520 - val_loss: 0.7192 - val_mse: 0.7192 - val_mae: 0.6248\n",
      "Epoch 2/20\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 16.7612 - mse: 16.7612 - mae: 3.7452 - val_loss: 29.9211 - val_mse: 29.9211 - val_mae: 5.2832\n",
      "Epoch 3/20\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 9.3125 - mse: 9.3125 - mae: 2.8960 - val_loss: 70.9549 - val_mse: 70.9549 - val_mae: 8.2669\n",
      "Epoch 4/20\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 16.0888 - mse: 16.0888 - mae: 3.6376 - val_loss: 79.8209 - val_mse: 79.8209 - val_mae: 8.8449\n",
      "Epoch 5/20\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 14.0087 - mse: 14.0087 - mae: 3.3937 - val_loss: 56.9675 - val_mse: 56.9675 - val_mae: 7.5139\n",
      "Epoch 6/20\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 9.3977 - mse: 9.3977 - mae: 2.8890 - val_loss: 43.5338 - val_mse: 43.5338 - val_mae: 6.5958\n",
      "Epoch 7/20\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 6.5381 - mse: 6.5381 - mae: 2.3773 - val_loss: 33.0805 - val_mse: 33.0805 - val_mae: 5.7449\n",
      "Epoch 8/20\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 5.8059 - mse: 5.8059 - mae: 2.0914 - val_loss: 24.8729 - val_mse: 24.8729 - val_mae: 4.9335\n",
      "Epoch 9/20\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 5.6820 - mse: 5.6820 - mae: 1.9937 - val_loss: 23.8822 - val_mse: 23.8822 - val_mae: 4.7377\n",
      "Epoch 10/20\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 5.3975 - mse: 5.3975 - mae: 1.9589 - val_loss: 29.8318 - val_mse: 29.8318 - val_mae: 5.1947\n",
      "Epoch 11/20\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 4.4483 - mse: 4.4483 - mae: 1.6010 - val_loss: 42.0706 - val_mse: 42.0706 - val_mae: 6.1162\n",
      "Epoch 12/20\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 4.0262 - mse: 4.0262 - mae: 1.5978 - val_loss: 57.8048 - val_mse: 57.8048 - val_mae: 7.1674\n",
      "Epoch 13/20\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 4.6028 - mse: 4.6028 - mae: 1.6924 - val_loss: 72.6938 - val_mse: 72.6938 - val_mae: 8.0520\n",
      "Epoch 14/20\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 4.3756 - mse: 4.3756 - mae: 1.6445 - val_loss: 79.3181 - val_mse: 79.3181 - val_mae: 8.3910\n",
      "Epoch 15/20\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 3.8553 - mse: 3.8553 - mae: 1.5803 - val_loss: 81.7023 - val_mse: 81.7023 - val_mae: 8.4922\n",
      "Epoch 16/20\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 3.2545 - mse: 3.2545 - mae: 1.4276 - val_loss: 80.6326 - val_mse: 80.6326 - val_mae: 8.4026\n",
      "Epoch 17/20\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 2.9838 - mse: 2.9838 - mae: 1.3431 - val_loss: 76.8527 - val_mse: 76.8527 - val_mae: 8.1569\n",
      "Epoch 18/20\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 2.8432 - mse: 2.8432 - mae: 1.3190 - val_loss: 72.9651 - val_mse: 72.9651 - val_mae: 7.9035\n",
      "Epoch 19/20\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 2.7084 - mse: 2.7084 - mae: 1.2325 - val_loss: 70.3492 - val_mse: 70.3492 - val_mae: 7.7186\n",
      "Epoch 20/20\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 2.6888 - mse: 2.6888 - mae: 1.2266 - val_loss: 69.0518 - val_mse: 69.0518 - val_mae: 7.6121\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_11 (Dense)            (None, 24)                216       \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 10)                250       \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 477\n",
      "Trainable params: 477\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Mean Absolute Error(MAE): 2.7954821100234986\n",
      "Pearson's correlation coefficient:  0.9999999999999999 3.42113882891801e-48\n",
      "R2 value: 0.028249668311984832\n"
     ]
    }
   ],
   "source": [
    "y_pred, y_exp = neuralN('../data3/singleDNA.xlsx')\n",
    "printAllAccMetrics(y_pred, y_exp)\n",
    "writePreds('SS',y_pred, y_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a393207b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 8)\n",
      "[[2.92279412e+01 1.69000000e+02 1.46000000e+02 3.37566300e+04\n",
      "  9.85635000e+03 1.57200000e+03 1.60000000e+01 1.70000000e+01]\n",
      " [2.66888151e+01 2.80000000e+02 2.28000000e+02 5.10655200e+04\n",
      "  2.20345300e+04 2.58500000e+03 3.30000000e+01 3.10000000e+01]\n",
      " [3.05732484e+01 1.06000000e+02 7.50000000e+01 1.93834700e+04\n",
      "  4.76428000e+03 9.13000000e+02 7.00000000e+00 1.50000000e+01]\n",
      " [2.86585366e+01 1.13000000e+02 5.80000000e+01 1.81680100e+04\n",
      "  6.52749000e+03 9.34000000e+02 1.20000000e+01 1.20000000e+01]\n",
      " [2.66666667e+01 5.60000000e+01 3.70000000e+01 7.24589000e+03\n",
      "  3.46307000e+03 4.62000000e+02 5.00000000e+00 5.00000000e+00]\n",
      " [1.48880105e+01 1.69000000e+02 1.78000000e+02 4.26050600e+04\n",
      "  1.79303200e+04 2.12000000e+03 2.80000000e+01 2.40000000e+01]\n",
      " [2.32616941e+01 2.29000000e+02 1.80000000e+02 4.39747200e+04\n",
      "  1.94032200e+04 2.27300000e+03 2.70000000e+01 3.00000000e+01]\n",
      " [2.20338983e+01 4.50000000e+01 4.80000000e+01 1.16649600e+04\n",
      "  1.86299000e+03 5.17000000e+02 3.00000000e+00 9.00000000e+00]\n",
      " [1.96721311e+01 8.80000000e+01 8.90000000e+01 2.49641100e+04\n",
      "  5.33109000e+03 1.02200000e+03 8.00000000e+00 1.60000000e+01]\n",
      " [2.62195122e+01 4.80000000e+01 4.80000000e+01 1.36812400e+04\n",
      "  0.00000000e+00 4.65000000e+02 0.00000000e+00 9.00000000e+00]\n",
      " [2.86792453e+01 1.71000000e+02 1.33000000e+02 2.65746800e+04\n",
      "  1.19445600e+04 1.49300000e+03 1.70000000e+01 2.00000000e+01]\n",
      " [1.58362989e+01 1.30000000e+02 1.32000000e+02 3.15779900e+04\n",
      "  8.51366000e+03 1.56900000e+03 1.90000000e+01 2.70000000e+01]\n",
      " [2.30870712e+01 2.11000000e+02 1.69000000e+02 1.76418100e+04\n",
      "  4.28396800e+04 2.14800000e+03 5.40000000e+01 1.50000000e+01]\n",
      " [3.40000000e+01 8.00000000e+01 5.10000000e+01 3.24033000e+03\n",
      "  8.70089000e+03 5.95000000e+02 1.10000000e+01 4.00000000e+00]\n",
      " [1.64948454e+01 4.50000000e+01 5.10000000e+01 1.74634200e+04\n",
      "  0.00000000e+00 5.43000000e+02 0.00000000e+00 8.00000000e+00]\n",
      " [2.37113402e+01 5.70000000e+01 6.70000000e+01 2.82819000e+03\n",
      "  1.28828200e+04 5.38000000e+02 1.50000000e+01 4.00000000e+00]\n",
      " [2.43362832e+01 6.40000000e+01 6.10000000e+01 1.47902900e+04\n",
      "  4.10863000e+03 6.41000000e+02 8.00000000e+00 8.00000000e+00]\n",
      " [2.78571429e+01 8.50000000e+01 5.80000000e+01 2.72662800e+04\n",
      "  0.00000000e+00 7.96000000e+02 0.00000000e+00 1.50000000e+01]\n",
      " [3.04761905e+01 1.51000000e+02 1.07000000e+02 5.01854000e+03\n",
      "  2.45021100e+04 1.20300000e+03 3.40000000e+01 7.00000000e+00]\n",
      " [2.60442260e+01 1.30000000e+02 1.08000000e+02 1.27981300e+04\n",
      "  1.18026400e+04 1.17000000e+03 1.90000000e+01 9.00000000e+00]\n",
      " [2.34299517e+01 1.16000000e+02 1.08000000e+02 1.74585200e+04\n",
      "  4.66733000e+03 1.16000000e+03 1.10000000e+01 1.60000000e+01]\n",
      " [2.72058824e+01 7.70000000e+01 7.10000000e+01 1.99219900e+04\n",
      "  5.19907000e+03 7.89000000e+02 6.00000000e+00 9.00000000e+00]\n",
      " [2.29249012e+01 2.18000000e+02 2.05000000e+02 3.00209900e+04\n",
      "  2.82313900e+04 2.16300000e+03 3.50000000e+01 2.00000000e+01]\n",
      " [2.47950820e+01 1.46000000e+02 1.28000000e+02 2.66867000e+04\n",
      "  1.08345300e+04 1.38000000e+03 2.10000000e+01 1.90000000e+01]]\n",
      "Epoch 1/20\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 44.1846 - mse: 44.1846 - mae: 5.8985 - val_loss: 8.2161 - val_mse: 8.2161 - val_mae: 2.3671\n",
      "Epoch 2/20\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 38.3708 - mse: 38.3708 - mae: 4.9226 - val_loss: 7.1211 - val_mse: 7.1211 - val_mae: 2.4122\n",
      "Epoch 3/20\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 34.3764 - mse: 34.3764 - mae: 4.5784 - val_loss: 5.3561 - val_mse: 5.3561 - val_mae: 1.9984\n",
      "Epoch 4/20\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 30.2333 - mse: 30.2333 - mae: 4.5001 - val_loss: 7.7572 - val_mse: 7.7572 - val_mae: 2.2840\n",
      "Epoch 5/20\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 28.1118 - mse: 28.1118 - mae: 4.4960 - val_loss: 11.1762 - val_mse: 11.1762 - val_mae: 2.8690\n",
      "Epoch 6/20\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 29.3334 - mse: 29.3334 - mae: 4.6375 - val_loss: 12.2718 - val_mse: 12.2718 - val_mae: 3.0753\n",
      "Epoch 7/20\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 28.7351 - mse: 28.7351 - mae: 4.5422 - val_loss: 9.4583 - val_mse: 9.4583 - val_mae: 2.5281\n",
      "Epoch 8/20\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 26.7344 - mse: 26.7344 - mae: 4.3140 - val_loss: 7.7347 - val_mse: 7.7347 - val_mae: 2.2834\n",
      "Epoch 9/20\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 26.0173 - mse: 26.0173 - mae: 4.2640 - val_loss: 6.8192 - val_mse: 6.8192 - val_mae: 2.1188\n",
      "Epoch 10/20\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 25.7676 - mse: 25.7676 - mae: 4.2252 - val_loss: 5.9595 - val_mse: 5.9595 - val_mae: 1.9247\n",
      "Epoch 11/20\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 25.6244 - mse: 25.6244 - mae: 4.1542 - val_loss: 5.8359 - val_mse: 5.8359 - val_mae: 1.9019\n",
      "Epoch 12/20\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 25.1976 - mse: 25.1976 - mae: 4.1392 - val_loss: 6.9472 - val_mse: 6.9472 - val_mae: 2.1429\n",
      "Epoch 13/20\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 24.8324 - mse: 24.8324 - mae: 4.1331 - val_loss: 7.9644 - val_mse: 7.9644 - val_mae: 2.3065\n",
      "Epoch 14/20\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 24.1631 - mse: 24.1631 - mae: 4.0564 - val_loss: 8.2107 - val_mse: 8.2107 - val_mae: 2.3371\n",
      "Epoch 15/20\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 23.9868 - mse: 23.9868 - mae: 4.0295 - val_loss: 9.4301 - val_mse: 9.4301 - val_mae: 2.5265\n",
      "Epoch 16/20\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 23.8296 - mse: 23.8296 - mae: 4.0661 - val_loss: 9.9736 - val_mse: 9.9736 - val_mae: 2.6662\n",
      "Epoch 17/20\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 23.5036 - mse: 23.5036 - mae: 4.0169 - val_loss: 9.5806 - val_mse: 9.5806 - val_mae: 2.5992\n",
      "Epoch 18/20\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 23.1635 - mse: 23.1635 - mae: 3.9492 - val_loss: 7.3931 - val_mse: 7.3931 - val_mae: 2.1358\n",
      "Epoch 19/20\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 22.4607 - mse: 22.4607 - mae: 3.8732 - val_loss: 7.0907 - val_mse: 7.0907 - val_mae: 2.0644\n",
      "Epoch 20/20\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 22.6182 - mse: 22.6182 - mae: 3.8547 - val_loss: 6.4686 - val_mse: 6.4686 - val_mae: 1.9590\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_23 (Dense)            (None, 8)                 72        \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 10)                90        \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 173\n",
      "Trainable params: 173\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Mean Absolute Error(MAE): 3.447058247009913\n",
      "Pearson's correlation coefficient:  1.0 0.0\n",
      "R2 value: -0.47810782649406325\n"
     ]
    }
   ],
   "source": [
    "y_pred, y_exp = neuralN('../data3/DNAI.xlsx')\n",
    "printAllAccMetrics(y_pred, y_exp)\n",
    "writePreds('DDI',y_pred, y_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368deea5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_pred, y_exp = neuralN('../data3/DNAII.xlsx')\n",
    "printAllAccMetrics(y_pred, y_exp)\n",
    "writePreds('DDII',y_pred, y_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df17f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_exp = neuralN('../data3/DNAIII.xlsx')\n",
    "printAllAccMetrics(y_pred, y_exp)\n",
    "writePreds('DDIII',y_pred, y_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76bd88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_exp = neuralN('../data3/MISC.xlsx')\n",
    "printAllAccMetrics(y_pred, y_exp)\n",
    "writePreds('MISC',y_pred, y_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d684d427",
   "metadata": {},
   "source": [
    "## Siamese Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bfd1215f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 8, 6)]       0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 8, 6)]       0           []                               \n",
      "                                                                                                  \n",
      " sequential_7 (Sequential)      (None, 2048)         281920      ['input_1[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 2048)         0           ['sequential_7[0][0]',           \n",
      "                                                                  'sequential_7[1][0]']           \n",
      "                                                                                                  \n",
      " dense_22 (Dense)               (None, 1)            2049        ['lambda[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 283,969\n",
      "Trainable params: 283,969\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#def siamese_single(filename):\n",
    "\n",
    "str_train,seq_train,exp_out = prepTrainSet('../data2/singleDNA.xlsx')\n",
    "\n",
    "for X_train in [str_train,seq_train]:\n",
    "    X = np.array(X_train)\n",
    "\n",
    "X = tf.reshape(X, [8,6])\n",
    "\n",
    "#input_shape=(8,6)\n",
    "               \n",
    "left_input = Input(X.shape)\n",
    "right_input = Input(X.shape)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv1D(64, kernel_size=3, activation='relu', input_shape=(X.shape)),\n",
    "    tf.keras.layers.MaxPooling1D(2, 2),\n",
    "    tf.keras.layers.Conv1D(128, kernel_size=2, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling1D(2, 2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(2048, activation='sigmoid')\n",
    "])\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "\n",
    "#subtracted = tf.keras.layers.Subtract()([encoded_l, encoded_r])\n",
    "output = Dense(1, activation='sigmoid')(L1_distance)\n",
    "model = Model(inputs=[left_input, right_input], outputs=output)\n",
    "\n",
    "optimizer= Adam(learning_rate=0.0006)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "58541baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/vidhi/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/vidhi/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/vidhi/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/vidhi/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\", line 808, in train_step\n        y_pred = self(x, training=True)\n    File \"/Users/vidhi/opt/anaconda3/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Users/vidhi/opt/anaconda3/lib/python3.8/site-packages/keras/engine/input_spec.py\", line 263, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"model\" is incompatible with the layer: expected shape=(None, 8, 6), found shape=(None, 6)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [38]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_out\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py:1129\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[1;32m   1130\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/vidhi/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/vidhi/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/vidhi/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/vidhi/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\", line 808, in train_step\n        y_pred = self(x, training=True)\n    File \"/Users/vidhi/opt/anaconda3/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Users/vidhi/opt/anaconda3/lib/python3.8/site-packages/keras/engine/input_spec.py\", line 263, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"model\" is incompatible with the layer: expected shape=(None, 8, 6), found shape=(None, 6)\n"
     ]
    }
   ],
   "source": [
    "model.fit([X,np.array(exp_out)], epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd3cb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#siamese_single('../data2/singleDNA.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28e33df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def siamese_dnaI(filename):\n",
    "    \n",
    "    str_train,seq_train,exp_out = prepTrainSet(filename)\n",
    "\n",
    "    for X_train in [str_train,seq_train]:\n",
    "        X = np.array(X_train)\n",
    "    X = X.reshape(24,6,1)\n",
    "\n",
    "    input_shape=(24,6)\n",
    "    left_input = Input(input_shape)\n",
    "    right_input = Input(input_shape)\n",
    "\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv1D(64, kernel_size=3, activation='relu',input_shape=input_shape),\n",
    "        tf.keras.layers.MaxPooling1D(2, 2),\n",
    "        tf.keras.layers.Conv1D(128, kernel_size=2, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling1D(2, 2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(4096, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    encoded_l = model(left_input)\n",
    "    encoded_r = model(right_input)\n",
    "\n",
    "    subtracted = tf.keras.layers.Subtract()([encoded_l, encoded_r])\n",
    "    prediction = Dense(1, activation='sigmoid')(subtracted)\n",
    "    siamese_net = Model(inputs=[left_input, right_input], outputs=prediction)\n",
    "\n",
    "    optimizer= Adam(learning_rate=0.0006)\n",
    "    siamese_net.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    siamese_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae29526",
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_dnaI('../data2/DNAI.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587a3d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def siamese_dnaII(filename):\n",
    "    \n",
    "    str_train,seq_train,exp_out = prepTrainSet(filename)\n",
    "\n",
    "    for X_train in [str_train,seq_train]:\n",
    "        X = np.array(X_train)\n",
    "    X = X.reshape(50,6,1)\n",
    "\n",
    "    input_shape=(50,6)\n",
    "    left_input = Input(input_shape)\n",
    "    right_input = Input(input_shape)\n",
    "\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv1D(64, kernel_size=3, activation='relu',input_shape=input_shape),\n",
    "        tf.keras.layers.MaxPooling1D(2, 2),\n",
    "        tf.keras.layers.Conv1D(128, kernel_size=2, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling1D(2, 2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(4096, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    encoded_l = model(left_input)\n",
    "    encoded_r = model(right_input)\n",
    "\n",
    "    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "    L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "\n",
    "    #subtracted = tf.keras.layers.Subtract()([encoded_l, encoded_r])\n",
    "    prediction = Dense(1, activation='sigmoid')(L1_distance)\n",
    "    siamese_net = Model(inputs=[left_input, right_input], outputs=prediction)\n",
    "\n",
    "    optimizer= Adam(learning_rate=0.0006)\n",
    "    siamese_net.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    siamese_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf3dfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_dnaII('../data2/DNAII.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df3a5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def siamese_dnaIII(filename):\n",
    "    \n",
    "    str_train,seq_train,exp_out = prepTrainSet(filename)\n",
    "\n",
    "    for X_train in [str_train,seq_train]:\n",
    "        X = np.array(X_train)\n",
    "    X = X.reshape(27,6,1)\n",
    "\n",
    "    input_shape=(27,6)\n",
    "    left_input = Input(input_shape)\n",
    "    right_input = Input(input_shape)\n",
    "\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv1D(64, kernel_size=3, activation='relu',input_shape=input_shape),\n",
    "        tf.keras.layers.MaxPooling1D(2, 2),\n",
    "        tf.keras.layers.Conv1D(128, kernel_size=2, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling1D(2, 2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(4096, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    encoded_l = model(left_input)\n",
    "    encoded_r = model(right_input)\n",
    "\n",
    "    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "    L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "\n",
    "    #subtracted = tf.keras.layers.Subtract()([encoded_l, encoded_r])\n",
    "    prediction = Dense(1, activation='sigmoid')(L1_distance)\n",
    "    siamese_net = Model(inputs=[left_input, right_input], outputs=prediction)\n",
    "\n",
    "    optimizer= Adam(learning_rate=0.0006)\n",
    "    siamese_net.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    siamese_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aae7c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_dnaIII('../data2/DNAIII.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf2a7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def siamese_misc(filename):\n",
    "    \n",
    "    str_train,seq_train,exp_out = prepTrainSet(filename)\n",
    "\n",
    "    for X_train in [str_train,seq_train]:\n",
    "        X = np.array(X_train)\n",
    "    X = X.reshape(8,6,1)\n",
    "\n",
    "    input_shape=(8,6)\n",
    "    left_input = Input(input_shape)\n",
    "    right_input = Input(input_shape)\n",
    "\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv1D(64, kernel_size=3, activation='relu',input_shape=input_shape),\n",
    "        tf.keras.layers.MaxPooling1D(2, 2),\n",
    "        tf.keras.layers.Conv1D(128, kernel_size=2, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling1D(2, 2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(4096, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    encoded_l = model(left_input)\n",
    "    encoded_r = model(right_input)\n",
    "\n",
    "    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "    L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "\n",
    "    #subtracted = tf.keras.layers.Subtract()([encoded_l, encoded_r])\n",
    "    prediction = Dense(1, activation='sigmoid')(L1_distance)\n",
    "    siamese_net = Model(inputs=[left_input, right_input], outputs=prediction)\n",
    "\n",
    "    optimizer= Adam(learning_rate=0.0006)\n",
    "    siamese_net.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    siamese_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc8dcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_misc('../data2/MISC.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82b7ed5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e0e6c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
